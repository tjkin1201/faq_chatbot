# í•„ìš”í•œ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
import os
import json
import streamlit as st
from typing import List, Any, Tuple
import requests
from requests.exceptions import ConnectionError
from dotenv import load_dotenv
from streamlit.runtime.uploaded_file_manager import UploadedFile

from langchain_community.chat_models import ChatOllama
from langchain_openai import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents.base import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain_core.runnables import Runnable
from langchain.schema.output_parser import StrOutputParser
from langchain_community.document_loaders import PyMuPDFLoader

# í˜ì´ì§€ ì„¤ì •ì„ ê°€ì¥ ë¨¼ì € í•´ì•¼ í•¨
st.set_page_config(
    page_title="ì²­ì•½ FAQ ì±—ë´‡",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

"""
PDF ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ
"""

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ìƒìˆ˜ ì •ì˜
OLLAMA_MODELS = {
    "Llama 3.2 (ìµœì‹  ì¶”ì²œ)": "llama3.2",
    "Qwen2.5 (ì¤‘êµ­ì–´/í•œêµ­ì–´ ìš°ìˆ˜)": "qwen2.5:7b",
    "Yi-1.5 (ë‹¤êµ­ì–´ ì„±ëŠ¥ ìš°ìˆ˜)": "yi:34b",
    "DeepSeek Coder (ì½”ë”© íŠ¹í™”)": "deepseek-coder:6.7b",
    "Mistral (í•œêµ­ì–´ ê°€ëŠ¥)": "mistral",
    "Llama2 (í•œêµ­ì–´ ê°€ëŠ¥)": "llama2",
    "Gemma (í•œêµ­ì–´ ê°€ëŠ¥)": "gemma:2b",
    "CodeLlama (ì½”ë”© íŠ¹í™”)": "codellama:7b",
    "Phi-3 (Microsoft ìµœì‹ )": "phi3:mini"
}

OPENAI_MODELS = {
    "GPT-4o (ìµœì‹  ì¶”ì²œ)": "gpt-4o",
    "GPT-4 Turbo": "gpt-4-turbo",
    "GPT-4": "gpt-4",
    "GPT-3.5 Turbo": "gpt-3.5-turbo"
}

EMBEDDING_MODELS = {
    "BGE-M3 (ë‹¤êµ­ì–´ ìµœê³ ì„±ëŠ¥)": "BAAI/bge-m3",
    "BGE-Large (í•œêµ­ì–´ ìš°ìˆ˜)": "BAAI/bge-large-en-v1.5",
    "E5-Large-V2 (ìµœì‹  ì¶”ì²œ)": "intfloat/e5-large-v2",
    "Multilingual-E5-Large": "intfloat/multilingual-e5-large",
    "Ko-SBERT-multitask": "jhgan/ko-sbert-multitask",
    "Ko-SBERT-nli (í•œêµ­ì–´ íŠ¹í™”)": "jhgan/ko-sbert-nli",
    "Jina-v2-Korean": "jinaai/jina-embeddings-v2-base-ko",
    "KoSimCSE-RoBERTa": "BM-K/KoSimCSE-roberta-multitask",
    "All-MPNet-Base-v2": "sentence-transformers/all-mpnet-base-v2",
    "Paraphrase-Multilingual": "sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
    "MiniLM-L6-v2 (ê¸°ë³¸)": "sentence-transformers/all-MiniLM-L6-v2"
}


def initialize_embeddings(model_name: str, use_cuda: bool) -> HuggingFaceEmbeddings:
    """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
    try:
        # meta tensor ë¬¸ì œë¥¼ í”¼í•˜ê¸° ìœ„í•´ CPUë¡œ ë¨¼ì € ì´ˆê¸°í™”
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODELS[model_name],
            model_kwargs={'device': 'cpu'}  # í•­ìƒ CPUë¡œ ì´ˆê¸°í™”
        )
        if use_cuda:
            st.info("GPU ì‚¬ìš©ì´ ì„ íƒë˜ì—ˆì§€ë§Œ, í˜¸í™˜ì„±ì„ ìœ„í•´ CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return embeddings
    except Exception as e:
        st.error(f"ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.error("ê¸°ë³¸ ì„ë² ë”© ëª¨ë¸ë¡œ ë‹¤ì‹œ ì‹œë„í•©ë‹ˆë‹¤.")
        try:
            # ê¸°ë³¸ ëª¨ë¸ë¡œ fallback
            embeddings = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'}
            )
            return embeddings
        except Exception as fallback_error:
            st.error(f"ê¸°ë³¸ ëª¨ë¸ë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(fallback_error)}")
            st.stop()


def check_ollama_status() -> Tuple[bool, str]:
    """Ollama ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    try:
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            installed_models = [
                model["name"] for model in response.json()["models"]
            ]
            return True, f"ì„¤ì¹˜ëœ ëª¨ë¸: {', '.join(installed_models)}"
        return False, "Ollama ì„œë¹„ìŠ¤ ì‘ë‹µ ì˜¤ë¥˜"
    except ConnectionError:
        return False, "Ollama ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
    except Exception as e:
        return False, f"ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {str(e)}"


def check_model_installed(model_name: str) -> bool:
    """Ollama ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    try:
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            installed_models = []
            for model in response.json()["models"]:
                installed_models.append(model["name"])
            return model_name in installed_models
        return False
    except Exception:
        return False


def install_model(model_name: str) -> bool:
    """Ollama ëª¨ë¸ ì„¤ì¹˜"""
    try:
        # ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ëª¨ë¸ ì„¤ì¹˜ ì§„í–‰ ìƒíƒœ ë°›ê¸°
        response = requests.post(
            "http://localhost:11434/api/pull",
            json={"name": model_name},
            stream=True
        )
        
        if response.status_code != 200:
            st.error(f"ëª¨ë¸ ì„¤ì¹˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: HTTP {response.status_code}")
            return False

        for line in response.iter_lines():
            if line:
                status = json.loads(line.decode())
                if 'status' in status:
                    st.write(f"ì§„í–‰ ì¤‘: {status['status']}")
                if 'error' in status:
                    st.error(f"ì„¤ì¹˜ ì˜¤ë¥˜: {status['error']}")
                    return False
        return True
    except requests.exceptions.ConnectionError:
        st.error("Ollama ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    except Exception as e:
        st.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False


def get_chat_model(model_type: str, model_name: str) -> Any:
    """ì±„íŒ… ëª¨ë¸ ì´ˆê¸°í™”"""
    if model_type == "Ollama":
        # Ollama ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
        status, message = check_ollama_status()
        if not status:
            st.error(f"""
            Ollama ì—°ê²° ì˜¤ë¥˜: {message}
            
            í•´ê²° ë°©ë²•:
            1. WSL2ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ Ollama ì„œë¹„ìŠ¤ë¥¼ ì‹œì‘í•˜ì„¸ìš”:
               `sudo systemctl start ollama`
               ë˜ëŠ”
               `ollama serve`
            
            2. Ollama ì„œë¹„ìŠ¤ê°€ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”:
               `curl http://localhost:11434/api/tags`
            """)
            st.stop()

        selected_model = OLLAMA_MODELS[model_name]
        
        # ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if not check_model_installed(selected_model):
            with st.spinner(f"{selected_model} ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                if not install_model(selected_model):
                    st.error(f"""
                    {selected_model} ëª¨ë¸ ì„¤ì¹˜ ì‹¤íŒ¨!
                    
                    ìˆ˜ë™ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:
                    `ollama pull {selected_model}`
                    """)
                    st.stop()
                else:
                    st.success(f"{selected_model} ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì—ˆìŠµë‹ˆë‹¤!")

        return ChatOllama(
            model=selected_model,
            temperature=0,
            base_url="http://localhost:11434",
            timeout=120
        )
    else:  # OpenAI
        return ChatOpenAI(
            model=OPENAI_MODELS[model_name],
            temperature=0
        )


def save_uploadedfile(uploadedfile: UploadedFile) -> str:
    """ì„ì‹œí´ë”ì— íŒŒì¼ ì €ì¥"""
    temp_dir = "PDF_ì„ì‹œí´ë”"
    if not os.path.exists(temp_dir):
        os.makedirs(temp_dir)
    file_path = os.path.join(temp_dir, uploadedfile.name)
    with open(file_path, "wb") as f:
        f.write(uploadedfile.read())
    return file_path


def pdf_to_documents(pdf_path: str) -> List[Document]:
    """ì €ì¥ëœ PDF íŒŒì¼ì„ Documentë¡œ ë³€í™˜"""
    documents = []
    loader = PyMuPDFLoader(pdf_path)
    doc = loader.load()
    for d in doc:
        d.metadata['file_path'] = pdf_path
    documents.extend(doc)
    return documents


def chunk_documents(documents: List[Document]) -> List[Document]:
    """Documentë¥¼ ë” ì‘ì€ documentë¡œ ë³€í™˜"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=400,  # í† í° ê¸°ì¤€ 300~512 ì‚¬ì´ ê¶Œì¥
        chunk_overlap=40,  # í† í° ê¸°ì¤€ 20~50 ì‚¬ì´ ê¶Œì¥
        length_function=len,
        is_separator_regex=False,
    )
    return text_splitter.split_documents(documents)


def save_to_vector_store(documents: List[Document]) -> None:
    """Documentë¥¼ ë²¡í„°DBë¡œ ì €ì¥"""
    vector_store = FAISS.from_documents(documents, embedding=embeddings)
    vector_store.save_local("faiss_index")


def get_rag_chain(model_type: str, model_name: str) -> Runnable:
    """RAG ì²´ì¸ ìƒì„±"""
    template = """
    ë‹¹ì‹ ì€ ì²­ì•½ ì œë„ì™€ ë¶€ë™ì‚° ì •ì±…ì— ëŒ€í•´ ì „ë¬¸ì ì¸ ì§€ì‹ì„ ê°–ì¶˜ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
    ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
    
    ë‹µë³€ í˜•ì‹ì„ ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•´ì£¼ì„¸ìš”:

    1. í•µì‹¬ ë‹µë³€: 
    - ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ "ì˜ˆ/ì•„ë‹ˆì˜¤" ë˜ëŠ” í•µì‹¬ ê²°ë¡ ì„ 1ë¬¸ì¥ìœ¼ë¡œ ì œì‹œ
    
    2. ìƒì„¸ ì„¤ëª…:
    - ê´€ë ¨ ë²•ê·œë‚˜ ê·œì • ì¸ìš©
    - êµ¬ì²´ì ì¸ ì‚¬ë¡€ë‚˜ ì¡°ê±´ ì„¤ëª…
    - ì˜ˆì™¸ ì‚¬í•­ì´ ìˆë‹¤ë©´ ì–¸ê¸‰
    
    3. ê´€ë ¨ ì£¼ì˜ì‚¬í•­:
    - ì‹¤ìˆ˜í•˜ê¸° ì‰¬ìš´ ë¶€ë¶„ ì•ˆë‚´
    - ìœ ì˜í•´ì•¼ í•  ë²•ì /ì œë„ì  ì‚¬í•­
    - ì¶”ê°€ í™•ì¸ì´ í•„ìš”í•œ ì‚¬í•­
    
    4. ì˜ˆì‹œ:
    - ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ì‚¬ë¡€ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€ì„ í’ë¶€í•˜ê²Œ ë§Œë“œì„¸ìš”
    
    ì§€ì¹¨:
    - í•­ìƒ ì •í™•í•˜ê³  ê°ê´€ì ì¸ ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”
    - ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ì œì™¸í•˜ì„¸ìš”
    - ì „ë¬¸ ìš©ì–´ëŠ” ë°˜ë“œì‹œ ì‰¬ìš´ ì„¤ëª…ì„ ë§ë¶™ì´ì„¸ìš”
    - ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ê³µì†í•œ ì–´íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
    - ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”
    
    ì»¨í…ìŠ¤íŠ¸:
    {context}

    ì§ˆë¬¸: {question}

    ë‹µë³€: """

    custom_rag_prompt = PromptTemplate.from_template(template)
    
    # ëª¨ë¸ ì´ˆê¸°í™” ì‹œ temperature ì¡°ì •
    if model_type == "Ollama":
        model = ChatOllama(
            model=OLLAMA_MODELS[model_name],
            temperature=0.1,  # ë” ì •í™•í•œ ë‹µë³€ì„ ìœ„í•´ ë‚®ì¶¤
            base_url="http://localhost:11434",
            timeout=120
        )
    else:  # OpenAI
        model = ChatOpenAI(
            model=OPENAI_MODELS[model_name],
            temperature=0
        )

    return custom_rag_prompt | model | StrOutputParser()


def display_reference_docs(docs: List[Document]) -> None:
    """ì°¸ê³  ë¬¸ì„œì˜ ìœ„ì¹˜ë¥¼ í‘œì‹œ"""
    with st.expander("ì°¸ê³ í•œ ë¬¸ì„œ ë³´ê¸°"):
        for i, doc in enumerate(docs, 1):
            page_info = doc.metadata.get('page_number', 'í˜ì´ì§€ ì •ë³´ ì—†ìŒ')
            title_info = doc.metadata.get('title', 'ëª©ì°¨ ì •ë³´ ì—†ìŒ')
            st.markdown(f"**ë¬¸ì„œ {i}:**")
            if title_info != 'ëª©ì°¨ ì •ë³´ ì—†ìŒ':
                st.write(f"ëª©ì°¨: {title_info}")
            if page_info != 'í˜ì´ì§€ ì •ë³´ ì—†ìŒ':
                st.write(f"í˜ì´ì§€: {page_info}")
            st.write(doc.page_content)

# ë¬¸ì„œ ëª©ì°¨ ì˜ˆì‹œ
# ë¬¸ì„œ 3:
# ëª©ì°¨:
# ì „ë§¤ì œí•œ ê´€ë ¨ Q&A
# Q426. ì£¼íƒì˜ ì „ë§¤ì œí•œì´ ë¬´ì—‡ì¸ê°€ìš”?
# Q427. ì „ë§¤ì œí•œì„ ì ìš©ë°›ëŠ” ëŒ€ìƒê³¼ ì‚¬ì—…ì£¼ì²´ì˜ ì˜ë¯¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
# Q428. 30ì„¸ëŒ€ ë¯¸ë§Œì˜ ì£¼íƒì„ ê³µê¸‰í•˜ë ¤ëŠ” ê²½ìš°ì—ë„ ì „ë§¤ì œí•œì„ ì ìš©ë°›ëŠ”ì§€?
# Q429. 30ì„¸ëŒ€ ì´ìƒ ê±´ì„¤í•˜ì§€ë§Œ ã€Œì£¼íƒë²• ì‹œí–‰ë ¹ã€ ì œ27ì¡°ì œ4í•­ì— í•´ë‹¹í•˜ì—¬
# ã€Œì£¼íƒë²•ã€ ì œ15ì¡°ì— ë”°ë¥¸ ì‚¬ì—…ê³„íšìŠ¹ì¸ì„ ë°›ì§€ ì•Šê³  ê±´ì¶•í—ˆê°€ë§Œìœ¼ë¡œ
# ì£¼íƒ ì™¸ì˜ ì‹œì„¤ê³¼ ì£¼íƒì„ ë™ì¼ ê±´ì¶•ë¬¼ë¡œ ê±´ì¶•(ì£¼ìƒë³µí•©)í•˜ëŠ” ê²½ìš°
# ì „ë§¤ì œí•œì„ ì ìš©ë°›ëŠ”ì§€?


def postprocess_response(response: str) -> str:
    """ëª¨ë¸ì˜ ë‹µë³€ì„ í›„ì²˜ë¦¬í•˜ì—¬ í’ˆì§ˆì„ í–¥ìƒ"""
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    response = response.strip()
    
    # ë¬¸ë²• ë° ì–´íˆ¬ ê°œì„  (ì˜ˆ: í•œêµ­ì–´ ë§ì¶¤ë²• ê²€ì‚¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš© ê°€ëŠ¥)
    # response = korean_spell_checker(response)
    
    # ë‹µë³€ í˜•ì‹ ê²€ì¦ ë° ìˆ˜ì •
    if not response.endswith("."):
        response += "."
    
    return response


@st.cache_data
def process_question(user_question: str, model_type: str, model_name: str):
    """ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ RAG ì²˜ë¦¬"""
    # ë²¡í„° DB í˜¸ì¶œ
    new_db = FAISS.load_local(
        "faiss_index",
        embeddings,
        allow_dangerous_deserialization=True
    )

    # ê´€ë ¨ ë¬¸ì„œ 3ê°œë¥¼ í˜¸ì¶œí•˜ëŠ” Retriever ìƒì„±
    retriever = new_db.as_retriever(search_kwargs={"k": 3})
    # ì‚¬ìš©ì ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ë¬¸ì„œ 3ê°œ ê²€ìƒ‰
    retrieve_docs: List[Document] = retriever.invoke(user_question)

    # RAG ì²´ì¸ ì„ ì–¸
    chain = get_rag_chain(model_type, model_name)
    # ì§ˆë¬¸ê³¼ ë¬¸ë§¥ì„ ë„£ì–´ì„œ ì²´ì¸ ê²°ê³¼ í˜¸ì¶œ
    response = chain.invoke({
        "question": user_question,
        "context": retrieve_docs
    })

    return response, retrieve_docs


def check_api_key() -> bool:
    """OpenAI API í‚¤ í™•ì¸"""
    api_key = os.getenv("OPENAI_API_KEY")
    return bool(api_key)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    st.title("PDF ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")

    # ì‚¬ì´ë“œë°”ì— ëª¨ë¸ ì„¤ì • ì¶”ê°€
    with st.sidebar:
        st.header("ëª¨ë¸ ì„¤ì •")
        
        # CUDA ì‚¬ìš© ì—¬ë¶€ ì„ íƒ
        use_cuda = st.checkbox(
            "CUDA ì‚¬ìš© (GPU)",
            value=False,
            help="NVIDIA RTX 4090ê³¼ ê°™ì€ GPUë¥¼ í™œìš©í•˜ë ¤ë©´ ì„ íƒí•˜ì„¸ìš”."
        )
        
        # ì„ë² ë”© ëª¨ë¸ ì„ íƒ
        embedding_model = st.selectbox(
            "ì„ë² ë”© ëª¨ë¸ ì„ íƒ",
            list(EMBEDDING_MODELS.keys()),
            help="ë¬¸ì„œ ë¶„ì„ì— ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”. BGE-M3ëŠ” ìµœì‹  ë‹¤êµ­ì–´ ëª¨ë¸ë¡œ ì„±ëŠ¥ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤."
        )
        
        # ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
        global embeddings
        embeddings = initialize_embeddings(embedding_model, use_cuda)
        
        # Ollama ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
        ollama_status, ollama_message = check_ollama_status()
        
        if ollama_status:
            st.success("Ollama ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤")
            st.info(ollama_message)
            default_model = "Ollama"
        else:
            st.warning("""
            Ollama ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
            OpenAIë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, WSL2ì—ì„œ Ollamaë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.
            
            ì„¤ì¹˜ ë°©ë²•:
            1. `curl https://ollama.ai/install.sh | sh`
            2. `systemctl --user start ollama`
            3. `ollama pull mistral`
            """)
            default_model = "OpenAI"
        
        model_type = st.radio(
            "ëª¨ë¸ ì¢…ë¥˜ ì„ íƒ",
            ["OpenAI", "Ollama"],
            index=0 if default_model == "OpenAI" else 1,
            help="OpenAIëŠ” API í‚¤ê°€ í•„ìš”í•˜ë©°, OllamaëŠ” ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” ë¬´ë£Œ ëª¨ë¸ì…ë‹ˆë‹¤."
        )

        if model_type == "Ollama":
            if not ollama_status:
                st.error("Ollama ì„œë¹„ìŠ¤ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
                st.stop()
            
            model_name = st.selectbox(
                "Ollama ëª¨ë¸ ì„ íƒ",
                list(OLLAMA_MODELS.keys()),
                help="í•œêµ­ì–´ ì²˜ë¦¬ì— ìµœì í™”ëœ ëª¨ë¸ë“¤ì…ë‹ˆë‹¤."
            )
        else:  # OpenAI
            if not check_api_key():
                st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                return
            
            model_name = st.selectbox(
                "OpenAI ëª¨ë¸ ì„ íƒ",
                list(OPENAI_MODELS.keys())
            )

    # ë©”ì¸ ì˜ì—­
    uploaded_file = st.file_uploader(
        "PDF íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”",
        type=['pdf']
    )

    if uploaded_file is not None:
        file_path = save_uploadedfile(uploaded_file)
        documents = pdf_to_documents(file_path)
        smaller_documents = chunk_documents(documents)
        save_to_vector_store(smaller_documents)
        st.success("PDF íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")

        user_question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
        
        if user_question:
            with st.spinner('ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...'):
                response, docs = process_question(
                    user_question, model_type, model_name
                )
                st.write(response)
                
                # ì°¸ê³ í•œ ë¬¸ì„œ ë³´ì—¬ì£¼ê¸°
                display_reference_docs(docs)


if __name__ == "__main__":
    main()
